{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Eksperimen Proyek Akhir SMSML\n",
                "## Membangun Sistem Machine Learning - Dicoding Indonesia\n",
                "\n",
                "**Nama**: Dafis Nadhif Saputra\n",
                "\n",
                "---\n",
                "\n",
                "Notebook ini berisi proses eksperimen lengkap mulai dari:\n",
                "1. Data Loading\n",
                "2. Exploratory Data Analysis (EDA)\n",
                "3. Data Preprocessing\n",
                "4. Feature Engineering\n",
                "5. Model Training (baseline)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import classification_report, confusion_matrix\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "print('Libraries imported successfully!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load your dataset\n",
                "# Ganti path dengan path ke dataset Anda\n",
                "# df = pd.read_csv('path/to/your/dataset.csv')\n",
                "\n",
                "# Contoh dengan dummy data\n",
                "np.random.seed(42)\n",
                "n_samples = 1000\n",
                "\n",
                "df = pd.DataFrame({\n",
                "    'feature_1': np.random.randn(n_samples),\n",
                "    'feature_2': np.random.randn(n_samples) * 2,\n",
                "    'feature_3': np.random.exponential(2, n_samples),\n",
                "    'feature_4': np.random.uniform(-1, 1, n_samples),\n",
                "    'feature_5': np.random.randint(0, 100, n_samples),\n",
                "    'category': np.random.choice(['A', 'B', 'C'], n_samples),\n",
                "    'target': np.random.choice([0, 1], n_samples, p=[0.6, 0.4])\n",
                "})\n",
                "\n",
                "print(f'Dataset shape: {df.shape}')\n",
                "print(f'Columns: {df.columns.tolist()}')\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Exploratory Data Analysis (EDA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Basic info\n",
                "print('=== Dataset Info ===')\n",
                "print(f'Shape: {df.shape}')\n",
                "print(f'\\nData Types:')\n",
                "print(df.dtypes)\n",
                "print(f'\\nMissing Values:')\n",
                "print(df.isnull().sum())\n",
                "print(f'\\nStatistical Summary:')\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "# Count plot\n",
                "df['target'].value_counts().plot(kind='bar', ax=axes[0], color=['steelblue', 'coral'])\n",
                "axes[0].set_title('Target Distribution')\n",
                "axes[0].set_xlabel('Class')\n",
                "axes[0].set_ylabel('Count')\n",
                "\n",
                "# Pie chart\n",
                "df['target'].value_counts().plot(kind='pie', autopct='%1.1f%%', ax=axes[1])\n",
                "axes[1].set_title('Target Proportion')\n",
                "axes[1].set_ylabel('')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribution of numerical features\n",
                "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
                "numerical_cols.remove('target')\n",
                "\n",
                "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, col in enumerate(numerical_cols):\n",
                "    if i < len(axes):\n",
                "        df[col].hist(bins=30, ax=axes[i], color='steelblue', edgecolor='black')\n",
                "        axes[i].set_title(f'Distribution of {col}')\n",
                "        axes[i].set_xlabel(col)\n",
                "        axes[i].set_ylabel('Frequency')\n",
                "\n",
                "# Hide empty subplots\n",
                "for j in range(i+1, len(axes)):\n",
                "    axes[j].set_visible(False)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation heatmap\n",
                "plt.figure(figsize=(10, 8))\n",
                "correlation = df[numerical_cols + ['target']].corr()\n",
                "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
                "plt.title('Correlation Heatmap')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Handle missing values (jika ada)\n",
                "print(f'Missing values sebelum handling:')\n",
                "print(df.isnull().sum())\n",
                "\n",
                "# Example: Fill numerical with median\n",
                "for col in numerical_cols:\n",
                "    if df[col].isnull().any():\n",
                "        df[col].fillna(df[col].median(), inplace=True)\n",
                "\n",
                "# Example: Fill categorical with mode\n",
                "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
                "for col in categorical_cols:\n",
                "    if df[col].isnull().any():\n",
                "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
                "\n",
                "print(f'\\nMissing values setelah handling:')\n",
                "print(df.isnull().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Encode categorical variables\n",
                "df_encoded = df.copy()\n",
                "\n",
                "label_encoders = {}\n",
                "for col in categorical_cols:\n",
                "    le = LabelEncoder()\n",
                "    df_encoded[col] = le.fit_transform(df_encoded[col])\n",
                "    label_encoders[col] = le\n",
                "    print(f'{col} encoded: {list(le.classes_)}')\n",
                "\n",
                "df_encoded.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split features and target\n",
                "X = df_encoded.drop('target', axis=1)\n",
                "y = df_encoded['target']\n",
                "\n",
                "print(f'Features shape: {X.shape}')\n",
                "print(f'Target shape: {y.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train-test split\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f'Training set: {X_train.shape}')\n",
                "print(f'Test set: {X_test.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature scaling\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "# Convert back to DataFrame\n",
                "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
                "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
                "\n",
                "print('Feature scaling complete!')\n",
                "X_train_scaled.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Baseline Model Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Random Forest classifier\n",
                "rf_model = RandomForestClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=10,\n",
                "    random_state=42,\n",
                "    n_jobs=-1\n",
                ")\n",
                "\n",
                "rf_model.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Predictions\n",
                "y_pred = rf_model.predict(X_test_scaled)\n",
                "\n",
                "print('Model training complete!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model evaluation\n",
                "print('=== Classification Report ===')\n",
                "print(classification_report(y_test, y_pred))\n",
                "\n",
                "print('\\n=== Confusion Matrix ===')\n",
                "cm = confusion_matrix(y_test, y_pred)\n",
                "plt.figure(figsize=(8, 6))\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
                "plt.title('Confusion Matrix')\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Actual')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature importance\n",
                "feature_importance = pd.DataFrame({\n",
                "    'feature': X.columns,\n",
                "    'importance': rf_model.feature_importances_\n",
                "}).sort_values('importance', ascending=False)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "sns.barplot(x='importance', y='feature', data=feature_importance, palette='viridis')\n",
                "plt.title('Feature Importance')\n",
                "plt.xlabel('Importance')\n",
                "plt.ylabel('Feature')\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "feature_importance"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Preprocessed Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save preprocessed data\n",
                "df_encoded.to_csv('preprocessed_data.csv', index=False)\n",
                "print('Preprocessed data saved to: preprocessed_data.csv')\n",
                "\n",
                "# Save scaler\n",
                "import joblib\n",
                "joblib.dump(scaler, 'scaler.joblib')\n",
                "print('Scaler saved to: scaler.joblib')\n",
                "\n",
                "# Save label encoders\n",
                "joblib.dump(label_encoders, 'label_encoders.joblib')\n",
                "print('Label encoders saved to: label_encoders.joblib')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## Kesimpulan\n",
                "\n",
                "Eksperimen ini telah menyelesaikan:\n",
                "1. ✅ Data loading dan inspeksi\n",
                "2. ✅ Exploratory Data Analysis\n",
                "3. ✅ Data preprocessing (handling missing values, encoding, scaling)\n",
                "4. ✅ Baseline model training dan evaluasi\n",
                "5. ✅ Menyimpan data yang sudah dipreprocess\n",
                "\n",
                "Selanjutnya, gunakan `modelling.py` atau `modelling_tuning.py` untuk training dengan MLflow tracking."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.12.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}